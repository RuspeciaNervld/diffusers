{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930b56c756b644d1be10af328c942543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /mnt/c/Users/11042/.cache/huggingface/hub/models--wyyadd--sd-1.5-inpainting/snapshots/fcac56836a29b84e1d03a5b08f2550890a510baa/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/c/Users/11042/.cache/huggingface/hub/models--wyyadd--sd-1.5-inpainting/snapshots/fcac56836a29b84e1d03a5b08f2550890a510baa/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "/home/nervld/miniconda3/envs/omini/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "An error occurred while trying to fetch /mnt/c/Users/11042/.cache/huggingface/hub/models--wyyadd--sd-1.5-inpainting/snapshots/fcac56836a29b84e1d03a5b08f2550890a510baa/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/c/Users/11042/.cache/huggingface/hub/models--wyyadd--sd-1.5-inpainting/snapshots/fcac56836a29b84e1d03a5b08f2550890a510baa/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_inpaint.StableDiffusionInpaintPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"/mnt/c/Users/11042/.cache/huggingface/hub/models--wyyadd--sd-1.5-inpainting/snapshots/fcac56836a29b84e1d03a5b08f2550890a510baa\",\n",
    "    revision=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None\n",
    ").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "# 加载lora\n",
    "lora_path = \"/home/nervld/gitclone/OminiControl/test/pytorch_lora_weights.safetensors\"  # 替换为你的 LoRA 模型路径\n",
    "pipe.load_lora_weights(lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a3620be4bb48c5acede590cc1b9432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\n",
    "image = Image.open(\"./datasets/inpainting/vton_hd/garment_person_pair_new/00024_00.jpg\")\n",
    "mask_image = Image.open(\"./datasets/inpainting/vton_hd/model_mask_pair/00024_00.jpg\")\n",
    "#image and mask_image should be PIL images.\n",
    "#The mask structure is white for inpainting and black for keeping as is\n",
    "image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\n",
    "image.save(\"./t_output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11647 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 100/11647 [02:52<5:31:39,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "# 把数据集里面的所有图片都进行inpainting然后取出\n",
    "# /home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/garment_person_pair_new里面是image，\n",
    "# /home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/model_mask_pair里面是mask\n",
    "# 把image和mask对应起来，进行inpainting\n",
    "# 把inpainting的结果保存到/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/garment_person_pair_new_inpainting里面\n",
    "\n",
    "# 获取文件列表\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "image_list = os.listdir(\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/garment_person_pair_new\")\n",
    "\n",
    "# 将result_list按照名称排序\n",
    "image_list = sorted(image_list)\n",
    "\n",
    "mask_list = os.listdir(\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/model_mask_pair\")\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "total_count = 100\n",
    "count = 0\n",
    "# 把image和mask对应起来，进行inpainting\n",
    "for image_file in tqdm(image_list):\n",
    "    if count >= total_count:\n",
    "        break\n",
    "    count += 1\n",
    "    image_path = os.path.join(\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/garment_person_pair_new\", image_file)\n",
    "    mask_path = os.path.join(\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/model_mask_pair\", image_file)\n",
    "    image = Image.open(image_path)\n",
    "    mask = Image.open(mask_path)\n",
    "    with torch.no_grad():\n",
    "        inpainting_result = pipe(prompt=\"\",image=image, mask_image=mask,num_inference_steps=20,guidance_scale=1.0,generator=torch.Generator(device=pipe.device).manual_seed(42)).images[0]\n",
    "    # 把图片切分为左右两张图，右边的图是正确结果\n",
    "    width, height = inpainting_result.size\n",
    "    right_image = inpainting_result.crop((width // 2, 0, width, height))\n",
    "    right_image.save(os.path.join(\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/new_result\", image_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将garment_person_pair_new_inpainting里面的图片切分为左右两张图，右边的图是正确结果\n",
    "real_image_list = os.listdir(\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/garment_person_pair_new\")\n",
    "real_image_list = sorted(real_image_list)\n",
    "\n",
    "count = 0\n",
    "total_count = 100\n",
    "\n",
    "for real_image_file in real_image_list:\n",
    "    if count >= total_count:\n",
    "        break\n",
    "    count += 1\n",
    "    real_image_path = os.path.join(\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/garment_person_pair_new\", real_image_file)\n",
    "    real_image = Image.open(real_image_path)\n",
    "    width, height = real_image.size\n",
    "    real_image = real_image.crop((width // 2, 0, width, height))\n",
    "    real_image = real_image.resize((256, 512))\n",
    "    real_image.save(os.path.join(\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/real_image\", real_image_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nervld/miniconda3/envs/omini/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/nervld/miniconda3/envs/omini/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nervld/miniconda3/envs/omini/lib/python3.10/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'numpy.ndarray' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     correct_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/real_image\u001b[39m\u001b[38;5;124m\"\u001b[39m, result_file))\n\u001b[1;32m     38\u001b[0m     ssim_list\u001b[38;5;241m.\u001b[39mappend(calculate_ssim(result_image, correct_image))\n\u001b[0;32m---> 39\u001b[0m     lpips_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcalculate_lpips\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect_image\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(ssim_list)\n",
      "Cell \u001b[0;32mIn[52], line 21\u001b[0m, in \u001b[0;36mcalculate_lpips\u001b[0;34m(image1, image2)\u001b[0m\n\u001b[1;32m     19\u001b[0m image1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image1)\n\u001b[1;32m     20\u001b[0m image2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image2)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss_fn_alex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/omini/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/omini/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/omini/lib/python3.10/site-packages/lpips/lpips.py:118\u001b[0m, in \u001b[0;36mLPIPS.forward\u001b[0;34m(self, in0, in1, retPerLayer, normalize)\u001b[0m\n\u001b[1;32m    115\u001b[0m     in1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m in1  \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# v0.0 - original release had a bug, where input was not scaled\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m in0_input, in1_input \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43min0\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_layer(in1)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (in0, in1)\n\u001b[1;32m    119\u001b[0m outs0, outs1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mforward(in0_input), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mforward(in1_input)\n\u001b[1;32m    120\u001b[0m feats0, feats1, diffs \u001b[38;5;241m=\u001b[39m {}, {}, {}\n",
      "File \u001b[0;32m~/miniconda3/envs/omini/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/omini/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/omini/lib/python3.10/site-packages/lpips/lpips.py:154\u001b[0m, in \u001b[0;36mScalingLayer.forward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43minp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshift\u001b[49m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'numpy.ndarray' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "# 分别使用SSIM, FID, LPIPS评估inpainting结果\n",
    "# 使用SSIM评估inpainting结果\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# 计算SSIM\n",
    "def calculate_ssim(image1, image2):\n",
    "    image1 = np.array(image1)\n",
    "    image2 = np.array(image2)\n",
    "    return ssim(image1, image2, win_size=3)\n",
    "\n",
    "# 使用LPIPS评估inpainting结果\n",
    "import lpips\n",
    "import lpips\n",
    "loss_fn_alex = lpips.LPIPS(net='alex') # best forward scores\n",
    "# loss_fn_vgg = lpips.LPIPS(net='vgg') # closer to \"traditional\" perceptual loss, when used for optimization\n",
    "\n",
    "def calculate_lpips(image1, image2):\n",
    "    image1 = np.array(image1)\n",
    "    image2 = np.array(image2)\n",
    "    return loss_fn_alex(image1, image2)\n",
    "\n",
    "result_path = \"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/new_result\"\n",
    "result_list = os.listdir(result_path)\n",
    "\n",
    "\n",
    "# 去/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/garment_person_pair_new里面找到与result_list对应的图片\n",
    "\n",
    "# 计算SSIM, FID, LPIPS\n",
    "ssim_list = []\n",
    "lpips_list = []\n",
    "\n",
    "for result_file in result_list:\n",
    "    result_path = os.path.join(result_path, result_file)\n",
    "    result_image = Image.open(result_path)\n",
    "    correct_image = Image.open(os.path.join(\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/real_image\", result_file))\n",
    " \n",
    "    ssim_list.append(calculate_ssim(result_image, correct_image))\n",
    "    lpips_list.append(calculate_lpips(result_image, correct_image))\n",
    "    break\n",
    "\n",
    "print(ssim_list)\n",
    "print(lpips_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID value: -5.061391325966724e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_fid import fid_score\n",
    "\n",
    "# 准备真实数据分布和生成模型的图像数据\n",
    "real_images_folder = '/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/real_image'\n",
    "generated_images_folder = '/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/new_result'\n",
    "\n",
    "# 加载预训练的Inception-v3模型\n",
    "inception_model = torchvision.models.inception_v3(pretrained=True)\n",
    "\n",
    "# 计算FID距离值\n",
    "fid_value = fid_score.calculate_fid_given_paths([real_images_folder, generated_images_folder],batch_size=100,device=\"cuda\",dims=2048,num_workers=1)\n",
    "print('FID value:', fid_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加上网络结构的改造尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√\n"
     ]
    }
   ],
   "source": [
    "# 从/home/nervld/gitclone/OminiControl/test/condition_image_encoder/pytorch_lora_weights.safetensors加载一个condition_image_encoder\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "image_encoder_id = \"/mnt/c/Users/11042/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/snapshots/32bd64288804d66eefd0ccbe215aa642df71cc41\"\n",
    "condition_image_encoder = CLIPVisionModelWithProjection.from_pretrained(image_encoder_id).to(device=\"cuda\")\n",
    "condition_image_encoder.eval()\n",
    "print(\"√\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型的keys odict_keys(['base_model.model.vision_model.embeddings.class_embedding', 'base_model.model.vision_model.embeddings.patch_embedding.weight', 'base_model.model.vision_model.embeddings.position_embedding.weight', 'base_model.model.vision_model.pre_layrnorm.weight', 'base_model.model.vision_model.pre_layrnorm.bias', 'base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.0.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.0.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.0.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.0.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.1.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.1.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.1.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.1.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.2.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.2.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.2.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.2.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.3.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.3.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.3.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.3.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.4.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.4.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.4.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.4.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.5.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.5.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.5.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.5.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.6.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.6.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.6.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.6.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.7.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.7.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.7.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.7.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.8.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.8.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.8.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.8.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.9.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.9.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.9.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.9.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.10.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.10.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.10.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.10.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.11.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.11.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.11.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.11.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.12.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.12.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.12.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.12.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.12.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.12.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.12.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.12.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.12.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.13.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.13.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.13.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.13.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.13.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.13.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.13.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.13.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.13.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.14.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.14.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.14.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.14.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.14.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.14.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.14.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.14.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.14.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.15.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.15.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.15.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.15.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.15.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.15.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.15.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.15.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.15.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.16.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.16.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.16.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.16.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.16.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.16.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.16.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.16.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.16.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.17.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.17.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.17.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.17.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.17.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.17.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.17.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.17.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.17.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.18.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.18.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.18.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.18.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.18.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.18.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.18.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.18.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.18.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.19.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.19.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.19.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.19.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.19.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.19.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.19.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.19.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.19.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.20.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.20.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.20.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.20.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.20.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.20.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.20.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.20.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.20.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.21.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.21.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.21.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.21.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.21.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.21.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.21.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.21.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.21.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.22.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.22.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.22.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.22.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.22.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.22.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.22.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.22.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.22.layer_norm2.bias', 'base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.weight', 'base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.bias', 'base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.23.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.23.layer_norm1.weight', 'base_model.model.vision_model.encoder.layers.23.layer_norm1.bias', 'base_model.model.vision_model.encoder.layers.23.mlp.fc1.weight', 'base_model.model.vision_model.encoder.layers.23.mlp.fc1.bias', 'base_model.model.vision_model.encoder.layers.23.mlp.fc2.weight', 'base_model.model.vision_model.encoder.layers.23.mlp.fc2.bias', 'base_model.model.vision_model.encoder.layers.23.layer_norm2.weight', 'base_model.model.vision_model.encoder.layers.23.layer_norm2.bias', 'base_model.model.vision_model.post_layernorm.weight', 'base_model.model.vision_model.post_layernorm.bias', 'base_model.model.visual_projection.weight'])\n",
      "需要加载的keys dict_keys(['vision_model.encoder.layers.0.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.lora_B.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.lora_A.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.lora_B.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.lora_A.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.lora_B.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.lora_A.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.lora_B.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.lora_A.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.lora_B.weight'])\n",
      "torch.Size([1, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_200786/599331741.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapter_weights = torch.load(lora_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "from peft import PeftModel, LoraConfig\n",
    "from safetensors.torch import load_file  # 用于加载 safetensors 格式的权重\n",
    "\n",
    "\n",
    "# 构造一个lora_config\n",
    "lora_config = LoraConfig(\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    lora_alpha=128,\n",
    "    r=128,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\",\"in_proj\",\"fc\",\"proj\",\"ln_1\",\"ln_2\",\"ln_3\",\"ln_post\",\"ln_pre\",\"ln_post_attn\",\"ln_pre_attn\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# 3. 创建 PeftModel，并绑定 LoRA 适配器（**不使用 load_adapter**）\n",
    "lora_model = PeftModel(condition_image_encoder, lora_config)\n",
    "\n",
    "\n",
    "\n",
    "# 2. 加载本地的 safetensors 格式的 LoRA 权重\n",
    "lora_path = \"/home/nervld/gitclone/OminiControl/test/condition_image_encoder/condition_image_encoder.pth\"  # 替换为你的 LoRA 文件路径\n",
    "adapter_weights = torch.load(lora_path)\n",
    "\n",
    "print(\"模型的keys\",lora_model.state_dict().keys())\n",
    "print(\"需要加载的keys\",adapter_weights.keys())\n",
    "\n",
    "# 3. 定义一个函数来将 LoRA 权重应用到模型上\n",
    "def apply_lora_to_model(model, adapter_weights):\n",
    "    model_state_dict = model.state_dict()\n",
    "    for adapter_key in adapter_weights.keys():\n",
    "        # 将 adapter_key 转换为模型中的对应键\n",
    "        if \"vision_model\" in adapter_key:\n",
    "            model_key = adapter_key.replace(\"vision_model\", \"base_model.model.vision_model\").replace(\"weight\", \"default.weight\")\n",
    "            \n",
    "        else:\n",
    "            continue  # 如果不是 vision_model 的权重，则跳过\n",
    "\n",
    "        # 检查模型中是否存在对应的键\n",
    "        if model_key in model_state_dict:\n",
    "            # 获取 LoRA 权重\n",
    "            lora_weight = adapter_weights[adapter_key]\n",
    "            # 获取模型中的权重\n",
    "            model_weight = model_state_dict[model_key]\n",
    "\n",
    "            # 检查权重维度是否匹配\n",
    "            if lora_weight.shape == model_weight.shape:\n",
    "                # 将 LoRA 权重加到模型权重上\n",
    "                model_weight += lora_weight.to(model_weight.device)\n",
    "            else:\n",
    "                print(f\"Warning: Shape mismatch for {model_key}. Skipping.\")\n",
    "        else:\n",
    "            print(f\"Warning: Key {model_key} not found in model state dict. Skipping.\")\n",
    "\n",
    "# 4. 应用 LoRA 权重\n",
    "apply_lora_to_model(lora_model, adapter_weights)\n",
    "\n",
    "# 5. 模型转移到设备上（例如 GPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lora_model.to(device)\n",
    "\n",
    "# 6. 示例：使用模型进行推理\n",
    "pixel_values = torch.randn(1, 3, 224, 224).to(device)  # 示例输入\n",
    "outputs = lora_model(pixel_values)\n",
    "print(outputs.image_embeds.shape)  # 预期输出 torch.Size([1, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_200786/882617722.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  projection.load_state_dict(torch.load(\"/home/nervld/gitclone/OminiControl/test/projection/projection.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 加载lora\n",
    "# lora_path = \"/home/nervld/gitclone/OminiControl/test/condition_image_encoder\"  # 替换为你的 LoRA 模型路径\n",
    "\n",
    "\n",
    "# # 加载本地lora\n",
    "# lora_path = \"/home/nervld/gitclone/OminiControl/test/condition_image_encoder/pytorch_lora_weights.safetensors\"  # 替换为你的 LoRA 模型路径\n",
    "# condition_image_encoder.load_adapter(lora_path, adapter_name=\"condition_image_encoder2\")\n",
    "\n",
    "# 从/home/nervld/gitclone/OminiControl/test/projection/projection.pth 加载一个projection\n",
    "projection = torch.nn.Sequential(\n",
    "        torch.nn.Linear(1024, 1024),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(1024, 768)\n",
    "    ).to(device=\"cuda\", dtype=pipe.dtype)\n",
    "projection.load_state_dict(torch.load(\"/home/nervld/gitclone/OminiControl/test/projection/projection.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 257, 768])\n",
      "torch.Size([1, 257, 768])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ab8236b7bc4e3da37ac1c13ddbefca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number = 0\n",
    "# 1. 加载测试图像和mask\n",
    "inpaint_init = Image.open(f\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/garment_person_pair_new/00{number:03d}_00.jpg\")\n",
    "mask_image = Image.open(f\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/model_mask_pair/00{number:03d}_00.jpg\")\n",
    "condition_image = Image.open(f\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/condition_garment/00{number:03d}_00.png\")\n",
    "\n",
    "\n",
    "# 2. 调整大小并确保格式正确\n",
    "height = width = 512\n",
    "inpaint_init = inpaint_init.resize((width, height))\n",
    "mask_image = mask_image.resize((width, height))\n",
    "condition_image = condition_image.resize((224, 224))  # CLIP需要224x224\n",
    "condition_image = condition_image.convert(\"RGB\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    init_image = pipe.image_processor.preprocess(\n",
    "        inpaint_init,\n",
    "        height=height,\n",
    "        width=width\n",
    "    ).to(device=pipe.device, dtype=pipe.dtype)\n",
    "    \n",
    "    # 4. 预处理mask\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    mask = torch.from_numpy(np.array(mask_image.convert(\"L\"))[None, None, ...])\n",
    "    mask = mask.to(device=pipe.device, dtype=pipe.dtype) / 255.0\n",
    "\n",
    "    # 5. 创建masked image\n",
    "    masked_image = init_image * (1 - mask)\n",
    "    # 移入cuda\n",
    "    masked_image = masked_image.to(device=pipe.device, dtype=pipe.dtype)\n",
    "    \n",
    "    # 6. 预处理condition image\n",
    "    condition_tensor = pipe.image_processor.preprocess(\n",
    "        condition_image,\n",
    "        height=224,\n",
    "        width=224\n",
    "    ).to(device=pipe.device, dtype=pipe.dtype)\n",
    "    \n",
    "    # 7. 获取condition embeddings\n",
    "    condition_image_embeds = lora_model(condition_tensor)\n",
    "    condition_image_embeds = condition_image_embeds.last_hidden_state.to(dtype=pipe.dtype)\n",
    "    condition_image_embeds = projection(condition_image_embeds)\n",
    "\n",
    "    # 移入cuda\n",
    "    condition_image_embeds = condition_image_embeds.to(device=pipe.device, dtype=pipe.dtype)\n",
    "    print(condition_image_embeds.shape)\n",
    "    # 拼接一个相同维度的全0张量\n",
    "    # condition_image_embeds = torch.cat([condition_image_embeds, torch.zeros_like(condition_image_embeds)], dim=1)\n",
    "    print(condition_image_embeds.shape)\n",
    "    # 8. 准备latents\n",
    "    num_inference_steps = 20\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    image = pipe(\n",
    "        image=masked_image,\n",
    "        mask_image=mask,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        generator=torch.Generator(device=pipe.device).manual_seed(42),\n",
    "        # prompt = \"\",\n",
    "        prompt_embeds=condition_image_embeds, # 将condition image的embeds作为prompt_embeds传入\n",
    "        direct_use_prompt_embeds=True, # 直接使用prompt_embeds\n",
    "        # 这里修改了__call__函数，用direct_use_prompt_embeds=True来直接使用prompt_embeds\n",
    "        guidance_scale=1.0, # 设置guidance_scale为1.0，是为了禁用classifier-free guidance\n",
    "    ).images[0]\n",
    "\n",
    "# 10. 保存结果\n",
    "image.save(\"./t_output1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train.model import InpaintingModel2\n",
    "\n",
    "def get_config(config_path):\n",
    "    import os\n",
    "    import yaml\n",
    "    assert config_path is not None, \"Please set the XFL_CONFIG environment variable\"\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "config = get_config(\"/home/nervld/gitclone/OminiControl/train/config/inpainting_512_new.yaml\")\n",
    "training_config = config[\"train\"]\n",
    "\n",
    "\n",
    "pl_module = InpaintingModel2(\n",
    "    inpainting_pipe_id=config[\"inpainting_model_path\"],\n",
    "    image_encoder_id=config[\"image_encoder_path\"],\n",
    "    exsist_unet_lora_path=config.get(\"exsist_unet_lora_path\", None),\n",
    "    exsist_image_encoder_lora_path=config.get(\"exsist_image_encoder_lora_path\", None),\n",
    "    exsist_projection_path=config.get(\"exsist_projection_path\", None),\n",
    "    lora_config=training_config[\"lora_config\"],\n",
    "    device=f\"cuda\",\n",
    "    dtype=getattr(torch, config[\"dtype\"]),\n",
    "    optimizer_config=training_config[\"optimizer\"],\n",
    "    model_config=config.get(\"model\", {}),\n",
    "    gradient_checkpointing=training_config.get(\"gradient_checkpointing\", False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6dfad250f54519a218101da037cc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. 加载测试图像和mask\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "number = 19\n",
    "\n",
    "inpaint_init = Image.open(f\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/garment_person_pair_new/00{number:03d}_00.jpg\")\n",
    "mask_image = Image.open(f\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/model_mask_pair/00{number:03d}_00.jpg\")\n",
    "condition_image = Image.open(f\"/home/nervld/gitclone/OminiControl/datasets/inpainting/vton_hd/condition_garment/00{number:03d}_00.png\")\n",
    "\n",
    "# 2. 调整大小并确保格式正确\n",
    "height = width = 512\n",
    "inpaint_init = inpaint_init.resize((width, height))\n",
    "mask_image = mask_image.resize((width, height))\n",
    "condition_image = condition_image.resize((224, 224))  # CLIP需要224x224\n",
    "condition_image = condition_image.convert(\"RGB\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 3. 预处理输入\n",
    "    init_image = pl_module.inpainting_pipe.image_processor.preprocess(\n",
    "        inpaint_init,\n",
    "        height=height,\n",
    "        width=width\n",
    "    ).to(device=pl_module.device, dtype=pl_module.dtype)\n",
    "    \n",
    "    # 4. 预处理mask\n",
    "    mask = torch.from_numpy(np.array(mask_image.convert(\"L\"))[None, None, ...])\n",
    "    mask = mask.to(device=pl_module.device, dtype=pl_module.dtype) / 255.0\n",
    "    \n",
    "    # 5. 创建masked image\n",
    "    masked_image = init_image * (1 - mask)\n",
    "    # 移入cuda\n",
    "    masked_image = masked_image.to(device=pl_module.device, dtype=pl_module.dtype)\n",
    "    \n",
    "    # 6. 预处理condition image\n",
    "    condition_tensor = pl_module.inpainting_pipe.image_processor.preprocess(\n",
    "        condition_image,\n",
    "        height=224,\n",
    "        width=224\n",
    "    ).to(device=pl_module.device, dtype=pl_module.dtype)\n",
    "    \n",
    "    # 7. 获取condition embeddings\n",
    "    condition_image_embeds = pl_module.condition_image_encoder(condition_tensor)\n",
    "    condition_image_embeds = condition_image_embeds.last_hidden_state\n",
    "    condition_image_embeds = pl_module.projection(condition_image_embeds)\n",
    "\n",
    "    # 移入cuda\n",
    "    condition_image_embeds = condition_image_embeds.to(device=pl_module.device, dtype=pl_module.dtype)\n",
    "    \n",
    "    # 8. 准备latents\n",
    "    num_inference_steps = 20\n",
    "    pl_module.inpainting_pipe.scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    # 9. 生成图像\n",
    "    image = pl_module.inpainting_pipe(\n",
    "        image=masked_image,\n",
    "        mask_image=mask,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        generator=torch.Generator(device=pl_module.device),\n",
    "        prompt_embeds=condition_image_embeds, # 将condition image的embeds作为prompt_embeds传入\n",
    "        direct_use_prompt_embeds=True, # 直接使用prompt_embeds\n",
    "        # 这里修改了__call__函数，用direct_use_prompt_embeds=True来直接使用prompt_embeds\n",
    "        guidance_scale=1.0 # 设置guidance_scale为1.0，是为了禁用classifier-free guidance\n",
    "    ).images[0]\n",
    "\n",
    "image.save(\"./t_output1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d94f2ead382487cb36ee06533d31810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"the person in the right is wearing clothing exactly matching the style shown in the left reference image\"\n",
    "image = Image.open(\"./datasets/inpainting/vton_hd/garment_person_pair_new/00203_00.jpg\")\n",
    "mask_image = Image.open(\"./datasets/inpainting/vton_hd/model_mask_pair/00203_00.jpg\")\n",
    "#image and mask_image should be PIL images.\n",
    "#The mask structure is white for inpainting and black for keeping as is\n",
    "image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\n",
    "image.save(\"./t_output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65808925c9c546fe9c0911652f885eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\n",
    "image = Image.open(\"./datasets/inpainting/vton_hd/garment_person_pair_new/00203_00.jpg\")\n",
    "mask_image = Image.open(\"./datasets/inpainting/vton_hd/model_mask_pair/00203_00.jpg\")\n",
    "#image and mask_image should be PIL images.\n",
    "#The mask structure is white for inpainting and black for keeping as is\n",
    "image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\n",
    "image.save(\"./t_output.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
